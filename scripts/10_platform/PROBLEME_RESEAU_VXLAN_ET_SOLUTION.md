# Probl√®me R√©seau K3s : VXLAN Bloqu√© et Solution Technique

**Date** : 2025-11-24  
**Contexte** : Module 10 - D√©ploiement Platform KeyBuzz (API, UI, My Portal)  
**Infrastructure** : K3s HA sur Hetzner Cloud (8 n≈ìuds : 3 masters + 5 workers)

---

## üî¥ Probl√®me Identifi√©

### Sympt√¥mes

1. **Erreurs 503 Service Temporarily Unavailable** sur les 3 URLs :
   - `https://platform.keybuzz.io`
   - `https://platform-api.keybuzz.io`
   - `https://my.keybuzz.io`

2. **Pods en √©tat Running** mais inaccessibles :
   ```bash
   kubectl get pods -n keybuzz
   # Tous les pods sont Running (1/1)
   # keybuzz-api-* : 3 pods Running
   # keybuzz-ui-* : 3 pods Running
   # keybuzz-my-ui-* : 3 pods Running
   ```

3. **Services ClusterIP non fonctionnels** :
   ```bash
   kubectl get svc -n keybuzz
   # Services cr√©√©s mais inaccessibles
   # keybuzz-api: ClusterIP 10.43.92.243:8080
   # keybuzz-ui: ClusterIP 10.43.102.230:80
   # keybuzz-my-ui: ClusterIP 10.43.232.70:80
   ```

### Diagnostic Technique

#### 1. Test de Connectivit√© Services ClusterIP

```bash
# Depuis un pod dans le cluster
kubectl exec -n keybuzz keybuzz-api-xxx -- curl http://10.43.92.243:8080/health
# R√©sultat : Timeout apr√®s 2+ minutes
```

**Conclusion** : Les Services ClusterIP ne sont pas routables.

#### 2. Test de Connectivit√© IPs Pods Directes

```bash
# IPs des pods API
kubectl get pods -n keybuzz -l app=platform-api -o jsonpath='{.items[*].status.podIP}'
# R√©sultat : 10.42.5.5 10.42.7.7 10.42.9.4

# Test depuis l'Ingress NGINX (hostNetwork=true)
kubectl exec -n ingress-nginx nginx-ingress-controller-xxx -- curl http://10.42.5.5:8080/health
# R√©sultat : Timeout apr√®s 5 secondes
```

**Conclusion** : Le r√©seau overlay (flannel) ne fonctionne pas.

#### 3. Test CoreDNS

```bash
# CoreDNS √©tait en CrashLoopBackOff (corrig√© depuis)
kubectl get pods -n kube-system | grep coredns
# Maintenant : Running

# Test r√©solution DNS
kubectl exec -n keybuzz test-pod -- nslookup keybuzz-api.keybuzz.svc.cluster.local
# R√©sultat : connection timed out; no servers could be reached
```

**Conclusion** : CoreDNS ne peut pas √™tre atteint car il utilise aussi le r√©seau overlay.

#### 4. Test Services NodePort

```bash
# Conversion en NodePort
kubectl patch svc keybuzz-api -n keybuzz --type='json' -p='[{"op":"replace","path":"/spec/type","value":"NodePort"}]'
# NodePort 30080 cr√©√©

# Test depuis un n≈ìud worker
ssh root@10.0.0.110 'curl http://localhost:30080/health'
# R√©sultat : Timeout
```

**Conclusion** : Les NodePorts ne fonctionnent pas non plus.

---

## üîç Cause Racine

### VXLAN Bloqu√© sur Hetzner Cloud

D'apr√®s la documentation existante (`SOLUTION_HOSTNETWORK.md`), **Hetzner Cloud bloque le protocole VXLAN** (port 8472/UDP), qui est utilis√© par Flannel (CNI de K3s) pour cr√©er le r√©seau overlay.

**Impact** :
- ‚ùå Services ClusterIP : Non fonctionnels (routage via kube-proxy n√©cessite le r√©seau overlay)
- ‚ùå Services NodePort : Non fonctionnels (m√™me raison)
- ‚ùå Communication inter-pods via IPs overlay : Non fonctionnelle
- ‚ùå CoreDNS : Ne peut pas √™tre atteint via le r√©seau overlay

**Architecture Actuelle** :
```
Ingress NGINX (hostNetwork=true) 
  ‚Üí Essaie d'atteindre Service ClusterIP (10.43.x.x)
    ‚Üí kube-proxy doit router via r√©seau overlay
      ‚Üí VXLAN bloqu√© ‚Üí √âchec
```

---

## ‚úÖ Solution D√©finitive : Remplacement Flannel par Calico IPIP

### ‚ùå Pourquoi hostNetwork sur les Apps est une Mauvaise Solution

**Probl√®mes avec hostNetwork sur les Deployments** :
- ‚ùå **Conflits de ports** : Impossible d'avoir plusieurs replicas sur le m√™me n≈ìud
- ‚ùå **Non scalable** : Pas de HPA possible
- ‚ùå **Port starvation** : Chaque app n√©cessite ses propres ports (API:8080, UI:80, My:80, Chatwoot:3000, etc.)
- ‚ùå **S√©curit√© r√©duite** : Acc√®s direct au r√©seau h√¥te
- ‚ùå **Incompatible multi-tenant** : Partage de ports impossible
- ‚ùå **Performance instable** : R√©seau partag√© avec l'h√¥te
- ‚ùå **Mont√©e en charge impossible** : Scaling horizontal bloqu√©

**Conclusion** : hostNetwork sur les apps est un hack qui ne tient pas la charge pour un SaaS comme KeyBuzz.

### ‚úÖ Solution Professionnelle : Calico IPIP

**Remplacer Flannel (VXLAN) par Calico (IPIP)** :
- ‚úÖ IPIP fonctionne sur Hetzner Cloud (pas de port bloqu√©)
- ‚úÖ R√©seau overlay pleinement fonctionnel
- ‚úÖ Services ClusterIP op√©rationnels
- ‚úÖ CoreDNS accessible
- ‚úÖ Compatible avec Deployments classiques
- ‚úÖ Scalable (HPA, multi-replicas)
- ‚úÖ Architecture Kubernetes native

**Architecture avec Calico IPIP** :
```
Ingress NGINX (DaemonSet + hostNetwork=true)
  ‚Üì
Services ClusterIP (10.43.x.x) - Fonctionnels
  ‚Üì
Calico IPIP Overlay Network
  ‚Üì
Pods (10.42.x.x) - Deployments classiques
```

### Proc√©dure de Correction

#### √âtape 1 : D√©sactiver Flannel

Sur tous les masters K3s, modifier `/etc/rancher/k3s/config.yaml` :
```yaml
flannel-backend: none
disable-network-policy: true
```

Puis red√©marrer K3s : `systemctl restart k3s`

#### √âtape 2 : Installer Calico

```bash
kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.27.0/manifests/calico.yaml
```

#### √âtape 3 : Configurer Calico en Mode IPIP

Patcher le DaemonSet Calico pour :
- `vxlanMode: Never`
- `ipipMode: Always`
- `natOutgoing: Enabled`

**Script disponible** : `fix_k3s_network_calico.sh`

#### R√©sultat

Apr√®s correction :
- ‚úÖ Services ClusterIP fonctionnels
- ‚úÖ CoreDNS accessible
- ‚úÖ Communication Pod-to-Pod fonctionnelle
- ‚úÖ Ingress ‚Üí Backends op√©rationnel
- ‚úÖ Deployments classiques (pas de hostNetwork)
- ‚úÖ Scalabilit√© assur√©e (HPA, multi-replicas)

---

## ‚ö†Ô∏è Contraintes et Limitations

### 1. Ports Uniques par N≈ìud

Avec `hostNetwork: true`, chaque port ne peut √™tre utilis√© qu'une seule fois par n≈ìud. Si plusieurs pods du m√™me Deployment sont sur le m√™me n≈ìud, ils partageront le m√™me port.

**Solution** : Utiliser un DaemonSet au lieu d'un Deployment pour garantir un pod par n≈ìud, OU utiliser `podAntiAffinity` pour √©viter la co-localisation.

### 2. S√©curit√©

Les pods avec `hostNetwork: true` ont acc√®s √† tous les ports du n≈ìud h√¥te. Il faut s'assurer que :
- Les ports utilis√©s ne sont pas d√©j√† utilis√©s par d'autres services
- Les pods ne peuvent pas √©couter sur des ports privil√©gi√©s (< 1024) sans privil√®ges

### 3. DNS

Avec `dnsPolicy: ClusterFirstWithHostNet`, les pods peuvent toujours utiliser CoreDNS pour la r√©solution DNS, mais CoreDNS doit lui-m√™me √™tre accessible (potentiellement aussi en hostNetwork si n√©cessaire).

### 4. Scalabilit√©

Avec un Deployment et `hostNetwork: true`, si vous avez 3 replicas et 5 workers, les pods peuvent se r√©partir sur les n≈ìuds. Mais si 2 pods se retrouvent sur le m√™me n≈ìud, ils partageront le m√™me port (conflit).

**Recommandation** : Utiliser `podAntiAffinity` :
```yaml
spec:
  template:
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - platform-api
            topologyKey: kubernetes.io/hostname
```

---

## üîÑ Alternatives Consid√©r√©es

### Alternative 1 : Services NodePort + Load Balancer Direct

**Principe** : Configurer le Load Balancer Hetzner pour pointer directement vers les NodePorts.

**Probl√®me** : Les NodePorts ne fonctionnent pas non plus car ils d√©pendent du r√©seau overlay.

### Alternative 2 : Utiliser un autre CNI

**Principe** : Remplacer Flannel par un CNI qui n'utilise pas VXLAN (ex: Calico avec IPIP, Cilium, etc.).

**Probl√®me** : 
- N√©cessite une reconfiguration compl√®te du cluster
- Risque de downtime
- Complexit√© √©lev√©e

### Alternative 3 : DaemonSets au lieu de Deployments

**Principe** : Utiliser des DaemonSets avec `hostNetwork: true` pour garantir un pod par n≈ìud.

**Avantages** :
- Pas de conflit de ports
- Distribution automatique sur tous les n≈ìuds
- Solution d√©j√† valid√©e dans `SOLUTION_HOSTNETWORK.md`

**Inconv√©nients** :
- Nombre de pods fixe (un par n≈ìud)
- Pas de scaling horizontal facile
- Si vous avez 5 workers, vous aurez toujours 5 pods (pas 3)

---

## üìã Plan d'Action Propos√©

### Option A : Deployments avec hostNetwork + podAntiAffinity (Recommand√©)

1. Modifier les 3 Deployments (`keybuzz-api`, `keybuzz-ui`, `keybuzz-my-ui`) :
   - Ajouter `hostNetwork: true`
   - Ajouter `dnsPolicy: ClusterFirstWithHostNet`
   - Ajouter `hostPort` identique √† `containerPort`
   - Ajouter `podAntiAffinity` pour √©viter la co-localisation

2. Les Services ClusterIP restent inchang√©s (ils pointeront automatiquement vers les IPs des n≈ìuds)

3. Les Ingress restent inchang√©s (ils utiliseront les Services ClusterIP qui fonctionneront maintenant)

**Avantages** :
- ‚úÖ Contr√¥le du nombre de replicas (3 pods comme souhait√©)
- ‚úÖ Services ClusterIP fonctionnels
- ‚úÖ Ingress fonctionnel
- ‚úÖ Pas de conflit de ports gr√¢ce √† podAntiAffinity

**Inconv√©nients** :
- ‚ö†Ô∏è Si un n≈ìud tombe, les pods ne seront pas automatiquement red√©ploy√©s sur un autre n≈ìud (sauf si vous avez plus de replicas que de n≈ìuds)

### Option B : DaemonSets avec hostNetwork (Solution Valid√©e)

1. Convertir les Deployments en DaemonSets
2. Utiliser `hostNetwork: true`
3. Utiliser Services NodePort (ou ClusterIP qui pointera vers les IPs des n≈ìuds)

**Avantages** :
- ‚úÖ Solution d√©j√† valid√©e et document√©e
- ‚úÖ Pas de conflit de ports (un pod par n≈ìud)
- ‚úÖ Haute disponibilit√© (un pod sur chaque worker)

**Inconv√©nients** :
- ‚ö†Ô∏è Nombre de pods fixe (5 pods si 5 workers, pas 3)
- ‚ö†Ô∏è Pas de scaling horizontal facile

---

## ‚úÖ R√©ponse de ChatGPT (Expert KeyBuzz)

### 1. KeyBuzz Platform peut-il fonctionner avec `hostNetwork: true` ?

**‚û°Ô∏è NON.** hostNetwork sur les apps est incompatible avec KeyBuzz :
- Conflits de ports (impossible pour API/UI/My)
- Scaling impossible (2 pods = crash)
- Performance instable
- Mont√©e en charge impossible

### 2. Quelle option recommandez-vous ?

**‚û°Ô∏è Solution D√©finitive : Calico IPIP**
- Remplacer Flannel par Calico (IPIP mode)
- Garder les apps en Deployment ClusterIP classique
- Garder l'Ingress en DaemonSet hostNetwork

### 3. Alternative technique ?

**‚û°Ô∏è Calico IPIP est la seule solution viable** pour Hetzner Cloud :
- Hetzner bloque UDP 8472 ‚Üí VXLAN KO
- Flannel + VXLAN = mort
- Seul Calico permet un overlay STABLE sans VXLAN

### 4. Impact sur les Modules 10-16 ?

**‚û°Ô∏è Apr√®s passage √† Calico :**
- ‚úÖ Plus besoin d'hostNetwork pour les apps
- ‚úÖ Plus besoin de DaemonSets
- ‚úÖ Module 10-16 fonctionnent comme n'importe quel cluster K8s
- ‚úÖ KeyBuzz peut scaler (HPA, multi-replicas, auto-healing)

---

## üìö R√©f√©rences

- `Infra/scripts/10_keybuzz/SOLUTION_HOSTNETWORK.md` : Solution valid√©e avec DaemonSets + hostNetwork
- `Infra/scripts/10_platform/10_platform_01_deploy_api.sh` : Script actuel de d√©ploiement API
- `Infra/scripts/10_platform/10_platform_02_deploy_ui.sh` : Script actuel de d√©ploiement UI
- `Infra/scripts/10_platform/10_platform_03_deploy_my.sh` : Script actuel de d√©ploiement My Portal

---

## üìù Notes Techniques Suppl√©mentaires

### √âtat Actuel du Cluster

```bash
# N≈ìuds
k3s-master-01 : 10.0.0.100
k3s-master-02 : 10.0.0.101
k3s-master-03 : 10.0.0.102
k3s-worker-01 : 10.0.0.110
k3s-worker-02 : 10.0.0.111
k3s-worker-03 : 10.0.0.112
k3s-worker-04 : 10.0.0.113
k3s-worker-05 : 10.0.0.114

# Ingress NGINX
- DaemonSet avec hostNetwork: true
- √âcoute sur ports 80/443 de tous les n≈ìuds
- NodePort : 31695 (HTTP/HTTPS)

# Load Balancer Hetzner
- Pointe vers les 5 workers (10.0.0.110-114)
- Port 31695 (HTTP/HTTPS)
```

### Configuration Flannel Actuelle

```bash
# Flannel utilise VXLAN par d√©faut dans K3s
# Port 8472/UDP n√©cessaire mais bloqu√© par Hetzner
# Pas de configuration alternative visible dans K3s
```

---

**Document cr√©√© le** : 2025-11-24  
**Auteur** : Auto (Agent IA)  
**Statut** : ‚úÖ Solution valid√©e par ChatGPT (Expert KeyBuzz)

**Solution D√©finitive** : Voir `SOLUTION_CALICO_IPIP.md` et `fix_k3s_network_calico.sh`

